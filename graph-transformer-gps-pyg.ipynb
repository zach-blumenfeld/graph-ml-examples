{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zach-blumenfeld/graph-ml-examples/blob/main/graph-transformer-gps-pyg.ipynb)\n",
    "\n",
    "Based off of [this code example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_gps.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\r\n",
      "  Using cached torch-2.2.1-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\r\n",
      "Collecting torch_geometric\r\n",
      "  Using cached torch_geometric-2.5.0-py3-none-any.whl.metadata (64 kB)\r\n",
      "Collecting filelock (from torch)\r\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from torch) (4.9.0)\r\n",
      "Collecting sympy (from torch)\r\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch)\r\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: jinja2 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from torch) (3.1.3)\r\n",
      "Collecting fsspec (from torch)\r\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting tqdm (from torch_geometric)\r\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting numpy (from torch_geometric)\r\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\r\n",
      "Collecting scipy (from torch_geometric)\r\n",
      "  Using cached scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\r\n",
      "Collecting aiohttp (from torch_geometric)\r\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\r\n",
      "Requirement already satisfied: requests in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\r\n",
      "Collecting pyparsing (from torch_geometric)\r\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Collecting scikit-learn (from torch_geometric)\r\n",
      "  Using cached scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from torch_geometric) (5.9.0)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\r\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.1.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\r\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\r\n",
      "  Using cached multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\r\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch_geometric)\r\n",
      "  Using cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\r\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch_geometric)\r\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from requests->torch_geometric) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from requests->torch_geometric) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zachblumenfeld/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages (from requests->torch_geometric) (2024.2.2)\r\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->torch_geometric)\r\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->torch_geometric)\r\n",
      "  Using cached threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Downloading torch-2.2.1-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.7/59.7 MB\u001B[0m \u001B[31m18.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\r\n",
      "Using cached aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl (387 kB)\r\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\r\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\r\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\r\n",
      "Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\r\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\r\n",
      "Using cached scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl (10.4 MB)\r\n",
      "Using cached scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\r\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\r\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\r\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\r\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Using cached multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Using cached threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\r\n",
      "Using cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\r\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, pyparsing, numpy, networkx, multidict, joblib, fsspec, frozenlist, filelock, async-timeout, yarl, torch, scipy, aiosignal, scikit-learn, aiohttp, torch_geometric\r\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 filelock-3.13.1 frozenlist-1.4.1 fsspec-2024.2.0 joblib-1.3.2 mpmath-1.3.0 multidict-6.0.5 networkx-3.2.1 numpy-1.26.4 pyparsing-3.1.1 scikit-learn-1.4.1.post1 scipy-1.12.0 sympy-1.12 threadpoolctl-3.3.0 torch-2.2.1 torch_geometric-2.5.0 tqdm-4.66.2 yarl-1.9.4\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch_geometric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Embedding,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, GPSConv, global_add_pool\n",
    "from torch_geometric.nn.attention import PerformerAttention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/feo9qle74kg48gy/molecules.zip?dl=1\n",
      "Extracting ./molecules.zip\n",
      "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/train.index\n",
      "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/val.index\n",
      "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/test.index\n",
      "Processing...\n",
      "Processing train dataset: 100%|██████████| 10000/10000 [00:01<00:00, 5041.52it/s]\n",
      "Processing val dataset: 100%|██████████| 1000/1000 [00:00<00:00, 3993.86it/s]\n",
      "Processing test dataset: 100%|██████████| 1000/1000 [00:00<00:00, 5200.26it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')\n",
    "train_dataset = ZINC('.', subset=True, split='train', pre_transform=transform)\n",
    "val_dataset = ZINC('.', subset=True, split='val', pre_transform=transform)\n",
    "test_dataset = ZINC('.', subset=True, split='test', pre_transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class RedrawProjection:\n",
    "    def __init__(self, model: torch.nn.Module,\n",
    "                 redraw_interval: Optional[int] = None):\n",
    "        self.model = model\n",
    "        self.redraw_interval = redraw_interval\n",
    "        self.num_last_redraw = 0\n",
    "\n",
    "    def redraw_projections(self):\n",
    "        if not self.model.training or self.redraw_interval is None:\n",
    "            return\n",
    "        if self.num_last_redraw >= self.redraw_interval:\n",
    "            fast_attentions = [\n",
    "                module for module in self.model.modules()\n",
    "                if isinstance(module, PerformerAttention)\n",
    "            ]\n",
    "            for fast_attention in fast_attentions:\n",
    "                fast_attention.redraw_projection_matrix()\n",
    "            self.num_last_redraw = 0\n",
    "            return\n",
    "        self.num_last_redraw += 1\n",
    "\n",
    "class GPS(torch.nn.Module):\n",
    "    def __init__(self, channels: int, pe_dim: int, num_layers: int,\n",
    "                 attn_type: str, attn_kwargs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_emb = Embedding(28, channels - pe_dim)\n",
    "        self.pe_lin = Linear(20, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(20)\n",
    "        self.edge_emb = Embedding(4, channels)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(channels, channels),\n",
    "                ReLU(),\n",
    "                Linear(channels, channels),\n",
    "            )\n",
    "            conv = GPSConv(channels, GINEConv(nn), heads=4,\n",
    "                           attn_type=attn_type, attn_kwargs=attn_kwargs)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels // 2),\n",
    "            ReLU(),\n",
    "            Linear(channels // 2, channels // 4),\n",
    "            ReLU(),\n",
    "            Linear(channels // 4, 1),\n",
    "        )\n",
    "        self.redraw_projection = RedrawProjection(\n",
    "            self.convs,\n",
    "            redraw_interval=1000 if attn_type == 'performer' else None)\n",
    "\n",
    "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
    "        x_pe = self.pe_norm(pe)\n",
    "        x = torch.cat((self.node_emb(x.squeeze(-1)), self.pe_lin(x_pe)), 1)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, batch, edge_attr=edge_attr)\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 41\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m total_error \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(loader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m101\u001B[39m):\n\u001B[0;32m---> 41\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     val_mae \u001B[38;5;241m=\u001B[39m test(val_loader)\n\u001B[1;32m     43\u001B[0m     test_mae \u001B[38;5;241m=\u001B[39m test(test_loader)\n",
      "Cell \u001B[0;32mIn[9], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     18\u001B[0m out \u001B[38;5;241m=\u001B[39m model(data\u001B[38;5;241m.\u001B[39mx, data\u001B[38;5;241m.\u001B[39mpe, data\u001B[38;5;241m.\u001B[39medge_index, data\u001B[38;5;241m.\u001B[39medge_attr,\n\u001B[1;32m     19\u001B[0m             data\u001B[38;5;241m.\u001B[39mbatch)\n\u001B[1;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m (out\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m-\u001B[39m data\u001B[38;5;241m.\u001B[39my)\u001B[38;5;241m.\u001B[39mabs()\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m---> 21\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m data\u001B[38;5;241m.\u001B[39mnum_graphs\n\u001B[1;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pygenv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "attn_kwargs = {'dropout': 0.5}\n",
    "model = GPS(channels=64, pe_dim=8, num_layers=10, attn_type='multihead',\n",
    "            attn_kwargs=attn_kwargs).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.redraw_projection.redraw_projections()\n",
    "        out = model(data.x, data.pe, data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.pe, data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
